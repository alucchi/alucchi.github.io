---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% if site.author.googlescholar %}
  <div class="wordwrap">You can find all my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %}

<h2>Selected List</h2>

  <h3>Training Dynamics of Deep Neural Networks (DNNs)</h3>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2411.15958" target="_blank" rel="noopener">
        Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/2206.03126" target="_blank" rel="noopener">
        Signal propagation in transformers: Theoretical perspectives and the role of rank collapse
      </a>
    </li>
  </ul>

  <h3>Loss Landscape of DNNs</h3>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2410.12455" target="_blank" rel="noopener">
        Loss Landscape Characterization of Neural Networks without Over-Parametrization
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/2411.02139" target="_blank" rel="noopener">
        Theoretical characterisation of the Gauss-Newton conditioning in Neural Networks
      </a>
    </li>
  </ul>

  <h3>Generalization</h3>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2410.17796" target="_blank" rel="noopener">
        A Comprehensive Analysis on the Learning Curve in Kernel Ridge Regression
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/2402.01297" target="_blank" rel="noopener">
        Characterizing Overfitting in Kernel Ridgeless Regression Through the Eigenspectrum
      </a>
    </li>
  </ul>

  <h3>Non-convex Optimization</h3>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1705.05933" target="_blank" rel="noopener">
        Sub-sampled Cubic Regularization for Non-convex Optimization
      </a>
    </li>
  </ul>

  <h3>DNN Architectural Design</h3>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1805.10694" target="_blank" rel="noopener">
        Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization
      </a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/2003.01652" target="_blank" rel="noopener">
        Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks
      </a>
    </li>
  </ul>

{% include base_path %}

<!-- New style rendering if publication categories are defined -->
{% if site.publication_category %}
  {% for category in site.publication_category  %}
    {% assign title_shown = false %}
    {% for post in site.publications reversed %}
      {% if post.category != category[0] %}
        {% continue %}
      {% endif %}
      {% unless title_shown %}
        <h2>{{ category[1].title }}</h2><hr />
        {% assign title_shown = true %}
      {% endunless %}
      {% include archive-single.html %}
    {% endfor %}
  {% endfor %}
{% else %}
  {% for post in site.publications reversed %}
    {% include archive-single.html %}
  {% endfor %}
{% endif %}



